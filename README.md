# Projeto: ETL Big Data Python
-----

## ğŸ“‹ Sobre

Este projeto demonstra como processar eficientemente **1 bilhÃ£o de linhas de dados** (~14GB) usando diferentes abordagens em Python. O desafio Ã© calcular estatÃ­sticas (mÃ­nimo, mÃ©dia e mÃ¡ximo) de temperaturas por estaÃ§Ã£o meteorolÃ³gica, comparando o desempenho de vÃ¡rias bibliotecas e tÃ©cnicas.

[![Projeto](https://img.shields.io/badge/Projeto-Big%20Data-blue?style=for-the-badge)](https://suajornadadedados.com.br/)
[![Python](https://img.shields.io/badge/Python-3.12+-green?style=for-the-badge\&logo=python)](https://python.org)
[![Pandas](https://img.shields.io/badge/Pandas-Data%20Processing-blue?style=for-the-badge\&logo=pandas)](https://pandas.pydata.org/)
[![Dask](https://img.shields.io/badge/Dask-Distributed-orange?style=for-the-badge)](https://dask.org/)
[![Polars](https://img.shields.io/badge/Polars-Fast%20DataFrames-red?style=for-the-badge)](https://pola-rs.github.io/polars/)
[![DuckDB](https://img.shields.io/badge/DuckDB-Analytics-yellow?style=for-the-badge)](https://duckdb.org/)

-----

## ğŸ“Š Fluxo do Projeto

```mermaid
graph TD
A[Arquivo CSV<br/>1 BilhÃ£o de linhas<br/>~14GB] --> B{Abordagem}

B --> |Python Puro| C1[20 min]
B --> |Pandas| C2[263 seg]
B --> |Duckdb| C3[155 seg]
B --> |PySpark| C4[14 seg]

C1 --> D[EstatÃ­sticas por EstaÃ§Ã£o<br/>Min, MÃ©dia, Max]
C2 --> D
C3 --> D
C4 --> D

style A fill:#e1f5ff
style C5 fill:#c8e6c9
style D fill:#fff3e0
```

-----

## ğŸ“ Estrutura do Projeto

```
02-python-big-data-processing/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ create_measurements.py    # Gera arquivo de teste com 1 bilhÃ£o de linhas
â”‚   â”œâ”€â”€ using_python.py            # ImplementaÃ§Ã£o em Python puro
â”‚   â”œâ”€â”€ using_pandas.py           # ImplementaÃ§Ã£o com Pandas
â”‚   â”œâ”€â”€ using_dask.py             # ImplementaÃ§Ã£o com Dask
â”‚   â”œâ”€â”€ using_polars.py           # ImplementaÃ§Ã£o com Polars
â”‚   â”œâ”€â”€ using_duckdb.py           # ImplementaÃ§Ã£o com DuckDB
â”‚   â””â”€â”€ using_bash_and_awk.sh      # ImplementaÃ§Ã£o em Bash + awk
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ measurements.txt          # Arquivo gerado com dados de teste
â”‚   â””â”€â”€ weather_stations.csv      # Lista de estaÃ§Ãµes meteorolÃ³gicas
â”œâ”€â”€ pyproject.toml                # DependÃªncias do projeto
â””â”€â”€ README.md                     # Este arquivo
```

-----

## TÃ©cnicas Apresentadas

### ELT (Extract, Load, Transform)

**ELT** Ã© a nova estratÃ©gia de tratamento de dados, o fluxo dela Ã© :
***Extract***: ExtraÃ§Ã£o dos dados
***Load***: DuplicaÃ§Ã£o das informaÃ§Ãµes (banco de dados) da etapa de **ExtraÃ§Ã£o**
***Transform***: Tratamento dos dados

Uma das ferraments mais utilizadas para transformaÃ§Ã£o de dados em SQL, hoje em dia Ã© o ***dbt-core***,
onde vocÃª geralmente aplica essa estratÃ©gia de ***ELT*** nele, carregando o seu banco de dados e aplicando seu tratamento atravÃ©s do ***dbt***

-----

#### ConsideraÃ§Ãµes
Apesar do alto custo de **storage**, devido a duplicaÃ§Ã£o do banco de dados, Ã© um mÃ©todo eficiente jÃ¡ que em casos de problemas, erros e informaÃ§Ãµes erradas, vocÃª falcilmente consegue indetificar essa questÃ£o sabendo se Ã© um problema no dataset ou no framework(dbt).
Esse mÃ©todo era inviavel antigamente, devido ao alto custo de store, 1GB chegava a custar milhÃµes de dolares

### ETL (Extract, Load, Transform)

**ETL** Ã© uma estratÃ©gia de tratamento de dados jÃ¡ antiga e ainda utilizada, o fluxo dela Ã©:
***Extract***: ExtraÃ§Ã£o dos dados
***Transform***: TransformaÃ§Ã£o das informaÃ§Ãµes, depois do carregamento dos dados na etapa **Extract**
***Load***: DisponibilizaÃ§Ã£o dos dados para consulmo

-----

#### ConsideraÃ§Ãµes
Esse modo de tratamento de dados Ã© amplamente usado, pela eficiencia. PorÃ©m em questÃ£o a problema de visualizaÃ§Ã£o de dados era um pouco mais complicada, devido a nÃ£o ser se os problemas proviam do dataset ou do framework utilizado para visualizar os dados, foi ai que veio a estratÃ©gia **ELT** mais pesada, porÃ©m mais fÃ¡cil de governar. 

---
## ConsideraÃ§Ãµes

O arquivo **weather_stations_sample.csv**, tem a lista de cidades em que os dados serÃ£o gerados pelo script **create_measurements.py**