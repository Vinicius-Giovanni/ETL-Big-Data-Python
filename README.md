# Projeto: ETL Big Data Python
-----

## ğŸ“‹ Sobre

Este projeto demonstra como processar eficientemente **1 bilhÃ£o de linhas de dados** (~14GB) usando diferentes abordagens em Python. O desafio Ã© calcular estatÃ­sticas (mÃ­nimo, mÃ©dia e mÃ¡ximo) de temperaturas por estaÃ§Ã£o meteorolÃ³gica, comparando o desempenho de vÃ¡rias bibliotecas e tÃ©cnicas.

[![Projeto](https://img.shields.io/badge/Projeto-Big%20Data-blue?style=for-the-badge)](https://suajornadadedados.com.br/)
[![Python](https://img.shields.io/badge/Python-3.12+-green?style=for-the-badge\&logo=python)](https://python.org)
[![Pandas](https://img.shields.io/badge/Pandas-Data%20Processing-blue?style=for-the-badge\&logo=pandas)](https://pandas.pydata.org/)
[![DuckDB](https://img.shields.io/badge/DuckDB-Analytics-yellow?style=for-the-badge)](https://duckdb.org/)
[![Spark](https://img.shields.io/badge/Spark-Fast%20DataFrames-red?style=for-the-badge)](https://docs.databricks.com/aws/en/)
-----

## ğŸ“Š Fluxo do Projeto

![alt text](images\fluxo_projeto.png)

-----

## ğŸ“ Estrutura do Projeto

```
02-python-big-data-processing/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ create_measurements.py    # Gera arquivo de teste com 1 bilhÃ£o de linhas
â”‚   â”œâ”€â”€ using_python.py            # ImplementaÃ§Ã£o em Python puro
â”‚   â”œâ”€â”€ using_pandas.py           # ImplementaÃ§Ã£o com Pandas
â”‚   â”œâ”€â”€ using_dask.py             # ImplementaÃ§Ã£o com Dask
â”‚   â”œâ”€â”€ using_polars.py           # ImplementaÃ§Ã£o com Polars
â”‚   â”œâ”€â”€ using_duckdb.py           # ImplementaÃ§Ã£o com DuckDB
â”‚   â””â”€â”€ using_bash_and_awk.sh      # ImplementaÃ§Ã£o em Bash + awk
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ measurements.txt          # Arquivo gerado com dados de teste
â”‚   â””â”€â”€ weather_stations.csv      # Lista de estaÃ§Ãµes meteorolÃ³gicas
â”œâ”€â”€ pyproject.toml                # DependÃªncias do projeto
â””â”€â”€ README.md                     # Este arquivo
```

-----

## TÃ©cnicas Apresentadas

### ELT (Extract, Load, Transform)

**ELT** Ã© a nova estratÃ©gia de tratamento de dados, o fluxo dela Ã© :
***Extract***: ExtraÃ§Ã£o dos dados
***Load***: DuplicaÃ§Ã£o das informaÃ§Ãµes (banco de dados) da etapa de **ExtraÃ§Ã£o**
***Transform***: Tratamento dos dados

Uma das ferraments mais utilizadas para transformaÃ§Ã£o de dados em SQL, hoje em dia Ã© o ***dbt-core***,
onde vocÃª geralmente aplica essa estratÃ©gia de ***ELT*** nele, carregando o seu banco de dados e aplicando seu tratamento atravÃ©s do ***dbt***

-----

**ConsideraÃ§Ãµes**

Apesar do alto custo de **storage**, devido a duplicaÃ§Ã£o do banco de dados, Ã© um mÃ©todo eficiente jÃ¡ que em casos de problemas, erros e informaÃ§Ãµes erradas, vocÃª falcilmente consegue indetificar essa questÃ£o sabendo se Ã© um problema no dataset ou no framework(dbt).
Esse mÃ©todo era inviavel antigamente, devido ao alto custo de store, 1GB chegava a custar milhÃµes de dolares

### ETL (Extract, Load, Transform)

**ETL** Ã© uma estratÃ©gia de tratamento de dados jÃ¡ antiga e ainda utilizada, o fluxo dela Ã©:
***Extract***: ExtraÃ§Ã£o dos dados
***Transform***: TransformaÃ§Ã£o das informaÃ§Ãµes, depois do carregamento dos dados na etapa **Extract**
***Load***: DisponibilizaÃ§Ã£o dos dados para consulmo

-----

**ConsideraÃ§Ãµes**

Esse modo de tratamento de dados Ã© amplamente usado, pela eficiencia. PorÃ©m em questÃ£o a problema de visualizaÃ§Ã£o de dados era um pouco mais complicada, devido a nÃ£o ser se os problemas proviam do dataset ou do framework utilizado para visualizar os dados, foi ai que veio a estratÃ©gia **ELT** mais pesada, porÃ©m mais fÃ¡cil de governar. 

---
**ConsideraÃ§Ãµes**

O arquivo **weather_stations_sample.csv**, tem a lista de cidades em que os dados serÃ£o gerados pelo script **create_measurements.py**

---

### Load Less Data (Carregar Menos Dados)
Carregue menos dados, dados desnecessÃ¡rio, colunas, linhas e informaÃ§Ãµes que nÃ£o serÃ£o utilizadas

### Use Efficient Datatypes (Uso Eficiente de Tipo de Dados)

- Tipo ***category***:
    No SQL, quando temos uma coluna de cidade por exemplo, fazemos sempre uma nova tabela dimensÃ£o_cidade, oque Ã© isso?
    Imagina que, na tabela principal os nomes das tabelas se repetem diversas vezes e toda vez que ela se repete, ela ocupa mais espaÃ§o na memÃ³ria.
    Portanto, Ã© extremamente Ãºtil criar uma dim_cidade, onde trocamos os valores em string da cidade, para nÂº int unitÃ¡rio. Assim, os valores das cidades que antes ocupavam mais memÃ³ria,
    agora ocupam apenas um caracterer da memÃ³ria.

    O tipo 'category' faz exatamente isso, ele categoriza os tipos de informaÃ§Ãµes

**Exemplo de uso:**

df['name_column'] = df['name_column'].astype('category')

- Tipo ***float32***:
    O python, por padrÃ£o usa o float64 que armazena 15-17 dÃ­gitos portanto ocupa mais memÃ³ria.
    Ã‰ recomendado usar o float32, que armazena menos dÃ­gitos. Esse tipo Ã© utilizando quando a velocidade Ã© mais importante que a precisÃ£o

**Exempo de uso:**

df['name_column'] = df['name_column'].astype('float32')

### Chunksize
    Chunksize Ã© uma estratÃ©gia onde, dividimos um dataset gigante em outros menores, para que a memÃ³ria nÃ£o seja estourada ela apenas aplicara as transformaÃ§Ãµes, leitura e etc em um "pedaÃ§o" de 
    cada vez do dataset gigante.
    Existem prÃ³s e contras dessa estratÃ©gia, o contra Ã© que demora mais para que o processa inteiro seja finalizado, porÃ©m o prÃ³ Ã© que o seu processa rodarÃ¡ sem que vocÃª se preocupe com a memÃ³ria da sua mÃ¡quina.

**ObservaÃ§Ãµes**
DuckDB e Spark jÃ¡ possuem uma estrutura semelhante a ideia de 'chunksize' com os seus dados, sem vocÃª declarar isso no seu cÃ³digo. Portanto essa estratÃ©gia nÃ£o precisa ser implementada em ambos, porÃ©m, ambos possuem estratÃ©gias diferentes:

- DuckDB: Trabalha com **Multiprocessamento**, ou seja, divide seu dataset em datasets menores, e cada dataset terÃ¡ o seu core especifico rodando ele.

- Spark: Trabalha **Processamento DistribuÃ­do**, ou seja, divide seu dataset em datasets menores, e cada dataset terÃ¡ o seu core e mÃ¡quina rodando ele. Um **Processamento DistribuÃ­do** Ã© quando hÃ¡ vÃ¡rias mÃ¡quinas realizando **multiprocessamentos**

---

# Resultados

---

#### Python Puro
![alt text](images\py.png)

---

#### Pandas
![alt text](images\panda.png)

---

#### Duckdb
![alt text](images\duck.png)