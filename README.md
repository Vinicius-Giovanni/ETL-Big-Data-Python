# Projeto: ETL Big Data Python
-----

## ğŸ“‹ Sobre

Este projeto demonstra como processar eficientemente **1 bilhÃ£o de linhas de dados** (~14GB) usando diferentes abordagens em Python. O desafio Ã© calcular estatÃ­sticas (mÃ­nimo, mÃ©dia e mÃ¡ximo) de temperaturas por estaÃ§Ã£o meteorolÃ³gica, comparando o desempenho de vÃ¡rias bibliotecas e tÃ©cnicas.

[![Projeto](https://img.shields.io/badge/Projeto-Big%20Data-blue?style=for-the-badge)](https://suajornadadedados.com.br/)
[![Python](https://img.shields.io/badge/Python-3.12+-green?style=for-the-badge\&logo=python)](https://python.org)
[![Pandas](https://img.shields.io/badge/Pandas-Data%20Processing-blue?style=for-the-badge\&logo=pandas)](https://pandas.pydata.org/)
[![DuckDB](https://img.shields.io/badge/DuckDB-Analytics-yellow?style=for-the-badge)](https://duckdb.org/)
[![Spark](https://img.shields.io/badge/Spark-Fast%20DataFrames-red?style=for-the-badge)](https://docs.databricks.com/aws/en/)
-----

## ğŸ“Š Fluxo do Projeto

![alt text](images/fluxo_projeto.png)

-----

## ğŸ“ Estrutura do Projeto

```
02-python-big-data-processing/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ create_measurements.py    # Gera arquivo de teste com 1 bilhÃ£o de linhas
â”‚   â”œâ”€â”€ using_python.py            # ImplementaÃ§Ã£o em Python puro
â”‚   â”œâ”€â”€ using_pandas.py           # ImplementaÃ§Ã£o com Pandas
â”‚   â”œâ”€â”€ using_dask.py             # ImplementaÃ§Ã£o com Dask
â”‚   â”œâ”€â”€ using_polars.py           # ImplementaÃ§Ã£o com Polars
â”‚   â”œâ”€â”€ using_duckdb.py           # ImplementaÃ§Ã£o com DuckDB
â”‚   â””â”€â”€ using_bash_and_awk.sh      # ImplementaÃ§Ã£o em Bash + awk
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ measurements.txt          # Arquivo gerado com dados de teste
â”‚   â””â”€â”€ weather_stations.csv      # Lista de estaÃ§Ãµes meteorolÃ³gicas
â”œâ”€â”€ pyproject.toml                # DependÃªncias do projeto
â””â”€â”€ README.md                     # Este arquivo
```

-----

## âš™ï¸ TÃ©cnicas e Conceitos Aplicados
### ğŸ”„ ETL vs ELT

**ğŸ”¹ ETL (Extract, Transform, Load)**

Fluxo tradicional:
1. Extract â†’ ExtraÃ§Ã£o dos dados

2. Transform â†’ TransformaÃ§Ã£o antes de carregar

3. Load â†’ DisponibilizaÃ§Ã£o para consumo

ğŸ“Œ CaracterÃ­sticas:

* TransformaÃ§Ã£o ocorre fora do banco.
* Menor custo de armazenamento.
* Bastante utilizado em pipelines tradicionais.
* Pode dificultar rastreabilidade quando hÃ¡ erro no dado transformado.

-----

**ğŸ”¹ ELT (Extract, Load, Transform)**

Fluxo moderno:
1. Extract â†’ ExtraÃ§Ã£o dos dados

2. Load â†’ Carregamento no banco (dados brutos)

3. Transform â†’ TransformaÃ§Ã£o dentro do banco

ğŸ“Œ CaracterÃ­sticas:

* Dados brutos ficam armazenados.
* Melhor governanÃ§a e auditoria.
* Mais fÃ¡cil identificar se o problema estÃ¡ no dataset ou na transformaÃ§Ã£o.
* Maior custo de storage.

Ferramenta comum nesse modelo: dbt-core (transformaÃ§Ãµes via SQL dentro do banco).

ğŸ“Œ ObservaÃ§Ã£o histÃ³rica:
Esse modelo se tornou viÃ¡vel com a reduÃ§Ã£o do custo de armazenamento ao longo dos anos.

-----

## ğŸ“‚ Load Less Data

Carregar apenas o necessÃ¡rio Ã© uma das maiores otimizaÃ§Ãµes possÃ­veis.

* Evite colunas desnecessÃ¡rias
* Evite linhas irrelevantes
* Reduza leitura de disco (I/O)
* Reduza uso de memÃ³ria
* Menos dados = menos custo computacional

-----

## ğŸ§  Uso Eficiente de Tipos de Dados

Escolher o tipo correto impacta diretamente memÃ³ria e performance.

ğŸ”¹ category

Ideal para colunas com muitos valores repetidos (ex: cidade, estaÃ§Ã£o).
Equivalente ao conceito de tabela dimensÃ£o no modelo dimensional.
Internamente, ele armazena um identificador numÃ©rico em vez da string repetida.

    df['coluna'] = df['coluna'].astype('category')

BenefÃ­cio:

* ReduÃ§Ã£o significativa de memÃ³ria
* OperaÃ§Ãµes de groupby mais rÃ¡pidas

-----

ğŸ”¹ float32 vs float64

Por padrÃ£o, Python usa float64 (maior precisÃ£o, maior consumo de memÃ³ria).
Se a precisÃ£o extrema nÃ£o for necessÃ¡ria, float32 Ã© mais leve e mais rÃ¡pido.

    df['coluna'] = df['coluna'].astype('float32')

Trade-off:

Menos precisÃ£o
Melhor performance

-----

ğŸ“¦ Processamento em Chunks

Dividir grandes volumes em partes menores evita estouro de memÃ³ria.

Em vez de carregar 14GB de uma vez, o processamento ocorre por blocos menores.

Vantagens:

Controle de memÃ³ria

ExecuÃ§Ã£o mais estÃ¡vel

Desvantagem:

Pode aumentar o tempo total de execuÃ§Ã£o

-----

## ğŸš€ Como Cada Engine Lida com Escala

Nem sempre Ã© necessÃ¡rio implementar chunksize manualmente.

**ğŸ¦† DuckDB**

* ExecuÃ§Ã£o vetorizada
* Multiprocessamento automÃ¡tico
* Paralelismo interno por padrÃ£o
Ele divide o trabalho entre mÃºltiplos cores da mÃ¡quina.

-----



---

# Resultados

---

#### Python Puro
![alt text](images/py.png)

---

#### Pandas
![alt text](images/panda.png)

---

#### Duckdb
![alt text](images/duck.png)